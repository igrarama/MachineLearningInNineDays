{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: Basic Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Notebook by Itay Hazan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Single-Variable Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(x) = 10 \\cdot (x^4 - x^3 - x^2 + x + 1) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a function that computes $f$ given $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f(x):\n",
    "    return 10 * (np.power(x,4) - np.power(x,3) - np.power(x,2) + x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot $f$ for $x \\in [-2,2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x119ad99b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.random.uniform(low=-2,high=2,size=100)\n",
    "plt.scatter(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a function that evaluates the derivative $\\frac{df}{dx}$ given $x$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    return 10 * (4 * np.power(x,3) - 3 * np.power(x,2) - 2 * x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a function that performs single-variable gradient descent. After every iteration, the function saves $x, f(x), df(x)$ into a numpy array. The function terminates either when num_iterations iterations passed or when the update to x was smaller than precision. When it terminates, it returns the array of \"history\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, df, num_iterations=5, x0=1, step_size=0.01, precision=0.01):\n",
    "    # TODO: terminate either when num_iterations iterations passed or when the update to x was smaller than precision\n",
    "    history = np.zeros([num_iterations, 3])\n",
    "    iterations = 0\n",
    "    previousX = x0\n",
    "    while True:\n",
    "        dfi = df(previousX)\n",
    "        history[iterations, :] = np.array([previousX, f(previousX), dfi])\n",
    "        iterations = iterations + 1\n",
    "        xi = previousX - (step_size * dfi)\n",
    "        if iterations >= num_iterations or abs(xi - previousX) < precision:\n",
    "            break\n",
    "        previousX = xi\n",
    "    return history # TODO: returns the numpy array where each row is [x, f(x), df(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run 5 iterations of GD using the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 10.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(f, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run 200 more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 10.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(f, df, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the function $f$ and the points that GD \"visited\" on the same axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Gradient Descent highly depends on the initialization point. One possible way of overcoming this is to run GD from several random starting points, and then choosing the lowest value found as our minimum.\n",
    "\n",
    "Sample 3 random starting points, run GD from each one, and plot GDs progress each time. Choose the best value found as your estimation of the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.44620738e+00  4.86148063e+01 -1.44811947e+02]\n",
      " [ 1.91209747e-03  1.00190843e+01  9.96164865e+00]\n",
      " [-9.77043890e-02  8.93773293e+00  1.16303953e+01]\n",
      " [-2.14008342e-01  7.52091178e+00  1.25141201e+01]\n",
      " [-3.39149544e-01  5.98067998e+00  1.17719265e+01]\n",
      " [-4.56868809e-01  4.73331743e+00  9.06103016e+00]\n",
      " [-5.47479110e-01  4.06725372e+00  5.39367043e+00]\n",
      " [-6.01415815e-01  3.85243495e+00  2.47597919e+00]\n",
      " [-6.26175607e-01  3.80988572e+00  9.39800143e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]] 0.0001\n",
      "[[ 0.4381039  11.98920443 -1.15662956]\n",
      " [ 0.4496702  11.97428288 -1.42251093]\n",
      " [ 0.46389531 11.95177259 -1.74068229]\n",
      " [ 0.48130213 11.91818442 -2.11581581]\n",
      " [ 0.50246029 11.86878889 -2.54902362]\n",
      " [ 0.52795053 11.79753583 -3.03470014]\n",
      " [ 0.55829753 11.69736697 -3.5560673 ]\n",
      " [ 0.5938582  11.56130524 -4.07980993]\n",
      " [ 0.6346563  11.38474018 -4.55149119]\n",
      " [ 0.68017121 11.16897949 -4.89562797]\n",
      " [ 0.72912749 10.92503965 -5.02640531]\n",
      " [ 0.77939154 10.67494987 -4.87367375]\n",
      " [ 0.82812828 10.44721166 -4.41936169]\n",
      " [ 0.8723219  10.26625031 -3.72322605]\n",
      " [ 0.90955416 10.14208157 -2.91118861]\n",
      " [ 0.93866605 10.06845671 -2.12402048]\n",
      " [ 0.95990625 10.03024248 -1.46165222]\n",
      " [ 0.97452277 10.01248989 -0.96133254]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]] 0.0001\n",
      "[[-0.06961422  9.25900491 11.23340573]\n",
      " [-0.18194827  7.92065935 12.40487307]\n",
      " [-0.305997    6.37787962 12.16484412]\n",
      " [-0.42764544  5.01127319  9.93816744]\n",
      " [-0.52702712  4.18750352  6.35238369]\n",
      " [-0.59055096  3.88280377  3.11031015]\n",
      " [-0.62165406  3.81479242  1.22984605]\n",
      " [-0.63395252  3.80454768  0.43086247]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]] 0.0001\n"
     ]
    }
   ],
   "source": [
    "xrandom = np.random.uniform(low=-2,high=2,size=3)\n",
    "print(gradient_descent(f, df, 20, xrandom[0]), 0.0001)\n",
    "print(gradient_descent(f, df, 20, xrandom[1]), 0.0001)\n",
    "print(gradient_descent(f, df, 20, xrandom[2]), 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Changing the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Note: this steps builds on step 1. Do not use your implementation of random restarts here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat step 1 with `step_size=1` and plot your results. Afterwards, repeat step 1 with `step_size=0.0001` and plot your results. \n",
    "\n",
    "What can you conclude on the importance of choosing the learning rate intelligently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Two-Variable Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ f(x_1,x_2) = x_1 ^2 + x_2 ^2 $.\n",
    "\n",
    "Repeat step 1 for this $f$. Note that now $df$ should return $\\nabla f$, the gradient of $f$, as a numpy array with two components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 0.8]\n",
      "[0.64 0.64]\n",
      "[0.512 0.512]\n",
      "[0.4096 0.4096]\n",
      "[0.32768 0.32768]\n",
      "[0.262144 0.262144]\n",
      "[0.2097152 0.2097152]\n",
      "[0.16777216 0.16777216]\n",
      "[0.13421773 0.13421773]\n",
      "[0.10737418 0.10737418]\n",
      "[0.08589935 0.08589935]\n",
      "[0.06871948 0.06871948]\n",
      "[0.05497558 0.05497558]\n",
      "[0.04398047 0.04398047]\n",
      "[0.03518437 0.03518437]\n",
      "[0.0281475 0.0281475]\n",
      "[0.022518 0.022518]\n",
      "[0.0180144 0.0180144]\n",
      "[0.01441152 0.01441152]\n",
      "[0.01152922 0.01152922]\n",
      "[0.00922337 0.00922337]\n",
      "[0.0073787 0.0073787]\n",
      "[0.00590296 0.00590296]\n",
      "[0.00472237 0.00472237]\n",
      "[0.00377789 0.00377789]\n",
      "[0.00302231 0.00302231]\n",
      "[0.00241785 0.00241785]\n",
      "[0.00193428 0.00193428]\n",
      "[0.00154743 0.00154743]\n",
      "[0.00123794 0.00123794]\n",
      "[0.00099035 0.00099035]\n",
      "[0.00079228 0.00079228]\n",
      "[0.00063383 0.00063383]\n",
      "[0.00050706 0.00050706]\n",
      "[0.00040565 0.00040565]\n",
      "[0.00032452 0.00032452]\n",
      "[0.00025961 0.00025961]\n",
      "[0.00020769 0.00020769]\n",
      "[0.00016615 0.00016615]\n",
      "[0.00013292 0.00013292]\n",
      "[0.00010634 0.00010634]\n",
      "[8.50705917e-05 8.50705917e-05]\n",
      "[6.80564734e-05 6.80564734e-05]\n",
      "[5.44451787e-05 5.44451787e-05]\n",
      "[4.3556143e-05 4.3556143e-05]\n",
      "[3.48449144e-05 3.48449144e-05]\n",
      "[2.78759315e-05 2.78759315e-05]\n",
      "[2.23007452e-05 2.23007452e-05]\n",
      "[1.78405962e-05 1.78405962e-05]\n",
      "[1.42724769e-05 1.42724769e-05]\n",
      "[1.14179815e-05 1.14179815e-05]\n",
      "[9.13438523e-06 9.13438523e-06]\n",
      "[7.30750819e-06 7.30750819e-06]\n",
      "[5.84600655e-06 5.84600655e-06]\n",
      "[4.67680524e-06 4.67680524e-06]\n",
      "[3.74144419e-06 3.74144419e-06]\n",
      "[2.99315535e-06 2.99315535e-06]\n",
      "[2.39452428e-06 2.39452428e-06]\n",
      "[1.91561943e-06 1.91561943e-06]\n",
      "[1.53249554e-06 1.53249554e-06]\n",
      "[1.22599643e-06 1.22599643e-06]\n",
      "[9.80797146e-07 9.80797146e-07]\n",
      "[7.84637717e-07 7.84637717e-07]\n",
      "[6.27710174e-07 6.27710174e-07]\n",
      "[5.02168139e-07 5.02168139e-07]\n",
      "[4.01734511e-07 4.01734511e-07]\n",
      "[3.21387609e-07 3.21387609e-07]\n",
      "[2.57110087e-07 2.57110087e-07]\n",
      "[2.0568807e-07 2.0568807e-07]\n",
      "[1.64550456e-07 1.64550456e-07]\n",
      "[1.31640365e-07 1.31640365e-07]\n",
      "[1.05312292e-07 1.05312292e-07]\n",
      "[8.42498333e-08 8.42498333e-08]\n",
      "[6.73998667e-08 6.73998667e-08]\n",
      "[5.39198933e-08 5.39198933e-08]\n",
      "[4.31359147e-08 4.31359147e-08]\n",
      "[3.45087317e-08 3.45087317e-08]\n",
      "[2.76069854e-08 2.76069854e-08]\n",
      "[2.20855883e-08 2.20855883e-08]\n",
      "[1.76684706e-08 1.76684706e-08]\n",
      "[1.41347765e-08 1.41347765e-08]\n",
      "[1.13078212e-08 1.13078212e-08]\n",
      "[9.04625697e-09 9.04625697e-09]\n",
      "[7.23700558e-09 7.23700558e-09]\n",
      "[5.78960446e-09 5.78960446e-09]\n",
      "[4.63168357e-09 4.63168357e-09]\n",
      "[3.70534686e-09 3.70534686e-09]\n",
      "[2.96427748e-09 2.96427748e-09]\n",
      "[2.37142199e-09 2.37142199e-09]\n",
      "[1.89713759e-09 1.89713759e-09]\n",
      "[1.51771007e-09 1.51771007e-09]\n",
      "[1.21416806e-09 1.21416806e-09]\n",
      "[9.71334446e-10 9.71334446e-10]\n",
      "[7.77067557e-10 7.77067557e-10]\n",
      "[6.21654046e-10 6.21654046e-10]\n",
      "[4.97323236e-10 4.97323236e-10]\n",
      "[3.97858589e-10 3.97858589e-10]\n",
      "[3.18286871e-10 3.18286871e-10]\n",
      "[2.54629497e-10 2.54629497e-10]\n",
      "[2.03703598e-10 2.03703598e-10]\n",
      "(array([2.54629497e-10, 2.54629497e-10]), 100)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.sum(x**2, axis=0) # np.power(x,2).sum(axis = 0)\n",
    "\n",
    "def df(x): \n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "def gradient_descent2(f, df, num_iterations=5, x0=np.array([1,1]), step_size=0.01, precision=0.01):\n",
    "    # TODO: terminate either when num_iterations iterations passed or when the update to x was smaller than precision\n",
    "    iterations = 0\n",
    "    previousX = x0\n",
    "    while True:\n",
    "        dfi = df(previousX)\n",
    "        iterations = iterations + 1\n",
    "        xi = previousX - (step_size * dfi)\n",
    "        print(xi)\n",
    "        if iterations >= num_iterations:\n",
    "            break\n",
    "        previousX = xi\n",
    "    return previousX, iterations # TODO: returns the numpy array where each row is [x, f(x), df(x)]\n",
    "\n",
    "print(gradient_descent2(f, df, 100, np.array([1,1]),0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 (optional): Additional playing around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to change $f$ in either step 1 or step 4, and repeat the step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
