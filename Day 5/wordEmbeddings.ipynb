{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings on Harry Potter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Notebook by Itay Hazan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Use gensim implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/63/5a4b694ac7d0dd0a7d061ba6af0dbd057379da21c7ea7efd44ae3299f87d/gensim-3.7.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.6MB 994kB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/c8/de7dcf34d4b5f2ae94fe1055e0d6418fb97a63c9dc3428edd264704983a2/smart_open-1.8.0.tar.gz (40kB)\n",
      "\u001b[K    100% |████████████████████████████████| 40kB 2.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Collecting bz2file (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/46/bf0020c9ac0e500ffdc794d64bd7ef0120f2c196a28544cac74c7df9e88e/boto3-1.9.116-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 3.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Collecting botocore<1.13.0,>=1.12.116 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/61/c9e508d0b5f19e5ea255ba4bdc2a82b9466f6019349e15f1dca2aa619d38/botocore-1.12.116-py2.py3-none-any.whl (5.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.3MB 2.0MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 5.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.116->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.116->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shmulikmerabichicvashvili/Library/Caches/pip/wheels/f7/a6/ff/9ab5842c14e50e95a06a4675b0b4a689c9cab6064dac2b01d0\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shmulikmerabichicvashvili/Library/Caches/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.116 botocore-1.12.116 bz2file-0.98 gensim-3.7.1 jmespath-0.9.4 s3transfer-0.2.0 smart-open-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads the entire Harry Potter series into a list, split by periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harry_potter_books():\n",
    "    books = []\n",
    "    for i in range(7):\n",
    "        with open('HarryPotter/{}.txt'.format(i+1)) as f:\n",
    "            books += f.read().split('.')\n",
    "            \n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = get_harry_potter_books()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function, that does some basic pre-processing on the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(books): \n",
    "    # TODO: lowercase\n",
    "    # TODO: remove all end-of-line characters\n",
    "    # TODO: remove all punctuation\n",
    "    # TODO: tokenize words (=split by whitespaces)\n",
    "    lst = []\n",
    "    lst = [list(gensim.utils.tokenize(book, lower=True)) for book in books]\n",
    "    \n",
    "    #return a list of lists: element i of the outer list is a list of word in the i'th book\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pre_processing(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first ten sentences (after pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'boy', 'who', 'lived', 'mr']\n",
      "['and', 'mrs']\n",
      "['dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much']\n",
      "['they', 'were', 'the', 'last', 'people', 'you', 'd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didn', 't', 'hold', 'with', 'such', 'nonsense']\n",
      "['mr']\n",
      "['dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'grunnings', 'which', 'made', 'drills']\n",
      "['he', 'was', 'a', 'big', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache']\n",
      "['mrs']\n",
      "['dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'craning', 'over', 'garden', 'fences', 'spying', 'on', 'the', 'neighbors']\n",
      "['the', 'dursleys', 'had', 'a', 'small', 'son', 'called', 'dudley', 'and', 'in', 'their', 'opinion', 'there', 'was', 'no', 'finer', 'boy', 'anywhere']\n",
      "['the', 'dursleys', 'had', 'everything', 'they', 'wanted', 'but', 'they', 'also', 'had', 'a', 'secret', 'and', 'their', 'greatest', 'fear', 'was', 'that', 'somebody', 'would', 'discover', 'it']\n",
      "['they', 'didn', 't', 'think', 'they', 'could', 'bear', 'it', 'if', 'anyone', 'found', 'out', 'about', 'the', 'potters']\n",
      "['mrs']\n",
      "['potter', 'was', 'mrs']\n",
      "['dursley', 's', 'sister', 'but', 'they', 'hadn', 't', 'met', 'for', 'several', 'years', 'in', 'fact', 'mrs']\n",
      "['dursley', 'pretended', 'she', 'didn', 't', 'have', 'a', 'sister', 'because', 'her', 'sister', 'and', 'her', 'good', 'for', 'nothing', 'husband', 'were', 'as', 'undursleyish', 'as', 'it', 'was', 'possible', 'to', 'be']\n",
      "['the', 'dursleys', 'shuddered', 'to', 'think', 'what', 'the', 'neighbors', 'would', 'say', 'if', 'the', 'potters', 'arrived', 'in', 'the', 'street']\n",
      "['the', 'dursleys', 'knew', 'that', 'the', 'potters', 'had', 'a', 'small', 'son', 'too', 'but', 'they', 'had', 'never', 'even', 'seen', 'him']\n",
      "['this', 'boy', 'was', 'another', 'good', 'reason', 'for', 'keeping', 'the', 'potters', 'away', 'they', 'didn', 't', 'want', 'dudley', 'mixing', 'with', 'a', 'child', 'like', 'that']\n",
      "['when', 'mr']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(books[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86672"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize a Word2Vec model from gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8337046, 11208760)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(books, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(books, total_examples=len(books),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ron', 0.6754653453826904),\n",
       " ('harry', 0.5371386408805847),\n",
       " ('krum', 0.33083343505859375),\n",
       " ('bravely', 0.315293550491333),\n",
       " ('he', 0.31319132447242737),\n",
       " ('cedric', 0.30808591842651367),\n",
       " ('crookshanks', 0.2866794168949127),\n",
       " ('greyback', 0.28020909428596497),\n",
       " ('gotcha', 0.2790670096874237),\n",
       " ('tentatively', 0.2658160924911499)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = lambda x: model.wv.word_vec(x)\n",
    "\n",
    "# w1 = \"lord\"\n",
    "model.wv.most_similar(positive=[wv(\"harry\")-wv(\"hermione\")+wv(\"ron\")])\n",
    "\n",
    "#model.wv.most_similar(positive=\"harry\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implementing CBOW with Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Setting things up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get a list of all unique words in the dataset, and sort them alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [item for sublist in books for item in sublist]\n",
    "all_words.sort()\n",
    "all_words = set(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an inverted index of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "word_list = list(all_words)\n",
    "for i in range(len(word_list)):\n",
    "    vocabulary[word_list[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the number of occurances of each word in our dataset (a histogram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = []\n",
    "# TODO: complete hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given the histogram $h$, write a function that returns a probability distribution over the words in the histgram such that a more popular word will have a higher probability of being chosen:\n",
    "$$ \\Pr [\\text{sampling $i$'th word}] = \\frac{hist[i]}{\\sum_i hist[i]} $$\n",
    "\n",
    "Remark: it is customary to take the elemets in the right-hand side of the equality to some power smaller than 1, e.g.:\n",
    "$$ \\Pr [\\text{sampling $i$'th word}] = \\frac{hist[i] ^{3/4}}{\\sum_i hist[i]^{3/4}} $$\n",
    "You may use this in your code as well (ampirically gives better performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Construct the train set\n",
    "We define the window size and the dimension of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "neg_sample_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train set will consist of labeled pairs: ` (x=(context, center), y=0/1`)`: \n",
    "\n",
    "To create the train set:\n",
    "\n",
    " 1. For every `window=(context, center)` of the input\n",
    "   1. Add the pair `(x=(context, center), y=1)` to the dataset.\n",
    "   1. Sample `neg_sample_size` words, `w_1, ..., w_k`, from the distribution we computed in step 2.1, and add the all the pairs `(x=(context, w_i), y=0)` to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Construct the neural net\n",
    "We are going to create the following network architecture for negative sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Negative Sampling Architecture](neg_sampling.png \"Negative Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 150 # dimension of the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that, given a word, returns the 10 most similar words to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
